{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c27AIsQiufDe"
   },
   "outputs": [],
   "source": [
    "# data manipulation/visualization\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0EHlDDLufDr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJhmm3NC1edF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AanrcmPiufDo"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kX_OSZGUufDq"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('https://raw.githubusercontent.com/JamesBond0014/NBA_ALLSTAR_Prediction/main/ASG_data.csv')\n",
    "\n",
    "names_and_teams = df_data[['PLAYER', 'TEAM']]\n",
    "for df in [df_data]:\n",
    "\n",
    "    # the % of team's games the player played in\n",
    "    # sometimes because of scheduling/trades, a player's indiviual GP may exceed their current team's, so we impose a ceiling of 1\n",
    "    df['Play Pct.'] = (df['GP'] / df['Team GP']).map(lambda pct : min(pct, 1))\n",
    "\n",
    "#     # nomalized via league average pace for that year\n",
    "    for col in ['PTS', 'REB', 'AST', 'STL', 'BLK', 'TOV', '3PM']:\n",
    "        df['Adjusted ' + col] = df[col] / df['Avg. Pace']\n",
    "\n",
    "train_data, test_data, test_years, train_years = [],[], [2020], []\n",
    "# for i in range(3):\n",
    "#     test_years.append(randint(1996, 2020))\n",
    "\n",
    "for index, row in df_data.iterrows():\n",
    "    if (row['Year'] in test_years):\n",
    "        test_data.append(row)\n",
    "    else:\n",
    "        train_data.append(row)\n",
    "data_by_year = {}\n",
    "\n",
    "for index, row in df_data.iterrows():\n",
    "    curr_year = row['Year']\n",
    "    if (curr_year in data_by_year):\n",
    "        data_by_year[curr_year].append(row)\n",
    "    else:\n",
    "        data_by_year[curr_year] = [row]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YpLdJ9tufDr"
   },
   "outputs": [],
   "source": [
    " features_full = [\n",
    "    'Adjusted PTS',\n",
    "    'Adjusted REB',\n",
    "    'Adjusted AST',\n",
    "    'Adjusted STL',\n",
    "    'Adjusted BLK',\n",
    "    'Adjusted TOV',\n",
    "    'Adjusted 3PM',\n",
    "    'DEFWS',\n",
    "    'TS%',\n",
    "    'USG%',\n",
    "    'PIE',\n",
    "    'Play Pct.',\n",
    "    'Team Conference Rank',\n",
    "    'Prior ASG Appearances',\n",
    "    'AS Last Year?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6O5nn_FufDt",
    "outputId": "bf62763a-c9b9-46a1-94c0-826f182c77e9"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBKnKq-wufDu",
    "outputId": "be6db13b-57e3-471b-c48b-c2890ce4a1f6"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_targets = np.array(pd.DataFrame(train_df['Selected?']))\n",
    "train_df = train_df.drop(columns=['Selected?'])\n",
    "train_df_filtered = train_df[features_full]\n",
    "\n",
    "train_df_filtered, train_targets = BorderlineSMOTE(random_state=0).fit_sample(train_df_filtered, train_targets) #np.array(train_targets_onehot)\n",
    "train_targets = torch.from_numpy(train_targets).long()\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_targets = torch.from_numpy(np.array(pd.DataFrame(test_df['Selected?']))).float()\n",
    "test_df = test_df.drop(columns=['Selected?'])\n",
    "test_df_filtered = test_df[features_full]\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_targets = torch.from_numpy(np.array(pd.DataFrame(test_df['Selected?']))).float()\n",
    "test_df = test_df.drop(columns=['Selected?'])\n",
    "test_df_filtered = test_df[features_full]\n",
    "\n",
    "data_by_year_tar = {}\n",
    "for i in data_by_year:\n",
    "    data_by_year[i] = pd.DataFrame(data_by_year[i])\n",
    "    data_by_year_tar[i] = torch.from_numpy(np.array(pd.DataFrame(data_by_year[i]['Selected?']))).long()\n",
    "    data_by_year[i] = data_by_year[i].drop(columns=['Selected?'])\n",
    "    data_by_year[i] = data_by_year[i][features_full]\n",
    "# data_by_year.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jtQmBR2zB8y"
   },
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module): # from https://towardsdatascience.com/implementing-tabnet-in-pytorch-fc977c383279\n",
    "    def __init__(self, dim=None):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(0, self.dim)\n",
    "        original_size = input.size()\n",
    "        input = input.reshape(input.size(0), -1)\n",
    "        input = input.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = input.size(dim)\n",
    "        \n",
    "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
    "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
    "        range = torch.arange(start=1, end=number_of_logits + 1, device=device,step=1, dtype=input.dtype).view(1, -1)\n",
    "        range = range.expand_as(zs)\n",
    "\n",
    "        bound = 1 + range * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
    "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
    "        zs_sparse = is_gt * zs\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(input)\n",
    "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "        return output\n",
    "    def backward(self, grad_output):\n",
    "        dim = 1\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7xEsmKcrsRl"
   },
   "outputs": [],
   "source": [
    "class GBN(nn.Module):\n",
    "    def __init__(self, in_size, batch_size=128, momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(in_size, momentum = momentum)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batches = x.chunk(x.shape[0]//self.batch_size, 0)\n",
    "        x_norm = []\n",
    "        for i, batch in enumerate(batches):\n",
    "            x_norm.append(self.batch_norm(batch))\n",
    "        return torch.cat(x_norm,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNgdge8FwSQG"
   },
   "outputs": [],
   "source": [
    "class AttentionTransformer(nn.Module): \n",
    "    def __init__(self, in_size, out_size,relaxation, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "        self.norm = GBN(out_size, batch_size)\n",
    "        self.activation = Sparsemax() # play with other ones\n",
    "        self.relaxation = relaxation\n",
    "    \n",
    "    def forward(self, x, prior):\n",
    "        x = self.linear(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        prior = prior*(self.relaxation-x)\n",
    "        return x, prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr5zNZiWBsuF"
   },
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self,in_size,out_size,linear=None,batch_size=128):\n",
    "        super().__init__()\n",
    "        if not linear: self.linear = nn.Linear(in_size, out_size*2) #*2 for \"folding\"\n",
    "        else: self.linear = linear\n",
    "        self.norm = GBN(out_size*2, batch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        size = x.shape[1]\n",
    "        firstHalf, secondHalf = x.chunk(2, dim=1)\n",
    "        return firstHalf*torch.sigmoid(secondHalf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpsJ_1ErH9Rr"
   },
   "outputs": [],
   "source": [
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,shared_layers,ind_n,batch_size=128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.ModuleList()\n",
    "        self.ind= nn.ModuleList()\n",
    "        for i, layer in enumerate(shared_layers):\n",
    "            if i==0: self.shared.append(GLU(input_size, output_size, layer, batch_size = batch_size))\n",
    "            else: self.shared.append(GLU(output_size, output_size, layer, batch_size = batch_size))\n",
    "\n",
    "        for i in range(ind_n):\n",
    "            if (i ==0 and not shared_layers):\n",
    "                self.ind.append(GLU(input_size, output_size, batch_size = batch_size))\n",
    "            else:\n",
    "                self.ind.append(GLU(output_size, output_size, batch_size = batch_size))\n",
    "        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x_old = torch.zeros_like(x)\n",
    "        for i, glu in enumerate(self.shared):\n",
    "            if i==0: x = glu(x)\n",
    "            else:\n",
    "                x_new = glu(x)\n",
    "                x = torch.add(x, x_new)\n",
    "  \n",
    "        for glu in self.ind:\n",
    "            x_new = glu(x)\n",
    "            x = torch.add(x, x_new)\n",
    "        return x*self.scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmTw-1LkIAhK"
   },
   "outputs": [],
   "source": [
    "class DecisionStep(nn.Module):\n",
    "    def __init__(self, input_size, attention_feature_size, final_feature_size, \n",
    "                 shared, n, relax, batch_size =128):\n",
    "        super().__init__()\n",
    "        self.feature = FeatureTransformer(input_size, attention_feature_size+ \n",
    "                                          final_feature_size, shared, n, \n",
    "                                          batch_size)\n",
    "        self.attention = AttentionTransformer(attention_feature_size, input_size,\n",
    "                                              relax, batch_size)\n",
    "    def forward(self, x, prev, prior):\n",
    "        attn_mask, prior = self.attention(prev, prior)\n",
    "        loss = ((-1)*attn_mask*torch.log(attn_mask+1e-10)).mean()\n",
    "        x_mask = x*attn_mask\n",
    "        x = self.feature(x_mask)\n",
    "        return x, loss, prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dddCcpbjNn6W"
   },
   "outputs": [],
   "source": [
    "class TabNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, share_n=2, ind_n=2, dec_n=4,\n",
    "                 device= 'cpu', relax = 3):\n",
    "        super().__init__()\n",
    "        self.attention_feature_size, self.final_feature_size = 64,64\n",
    "        self.device = device\n",
    "        a, f = self.attention_feature_size, self.final_feature_size\n",
    "        self.shared_layers = nn.ModuleList()\n",
    "        for i in range(share_n):\n",
    "            if i ==0 : self.shared_layers.append(nn.Linear(input_size, 2*(a+f)))\n",
    "            else: self.shared_layers.append(nn.Linear(a+f, 2*(a+f)))\n",
    "        self.feature = FeatureTransformer(input_size, a+f, self.shared_layers, \n",
    "                                          ind_n)\n",
    "        self.dec_steps = nn.ModuleList()\n",
    "        for i in range(dec_n):\n",
    "            self.dec_steps.append(DecisionStep(input_size, f, a, self.shared_layers,\n",
    "                                               ind_n, relax))\n",
    "        self.linear = nn.Linear(f, output_size)\n",
    "        self.norm = nn.BatchNorm1d(input_size)\n",
    "        self.activation = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device = self.device)\n",
    "        x = self.norm(x)\n",
    "        x_prev = self.feature(x)[:,self.final_feature_size:]\n",
    "        loss = torch.zeros(1).to(x.device)\n",
    "        out = torch.zeros(x.size(0), self.final_feature_size).to(x.device)\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "\n",
    "        for step in self.dec_steps:\n",
    "            x_out, lss, prior = step(x, x_prev,prior)\n",
    "            first, second = x_out.chunk(2, dim = 1)\n",
    "            out += F.relu(first)\n",
    "            x_prev = second\n",
    "            loss += lss\n",
    "        out = self.linear(out)\n",
    "        y_pred = self.activation(out)\n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDRf2PsoufDy"
   },
   "outputs": [],
   "source": [
    "train_tensor = torch.from_numpy(np.array(train_df_filtered)).float().to(device = device)\n",
    "test_tensor = torch.from_numpy(np.array(test_df_filtered)).float().to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DVOzpEZJufD0"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Bah5enIDb7iM",
    "outputId": "531ef866-ac34-4744-9fda-674d5627aad8"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZS2yHTxyKQN"
   },
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeiCo7jNEPil"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, losses):\n",
    "    res = model(test_tensor)\n",
    "    class_pred = torch.argmax(res,dim=1).cpu().detach().numpy()\n",
    "    class_target = test_targets.detach().numpy()\n",
    "\n",
    "    num_cor = len([True for i in range(0, len(class_target)) if class_pred[i] == class_target[i]])\n",
    "    tp = len([True for i in range(0, len(class_target)) if class_pred[i] != 0 and class_pred[i] == class_target[i]])\n",
    "    fn = len([True for i in range(0, len(class_target)) if class_pred[i] == 0 and class_pred[i] != class_target[i]])\n",
    "    p = len([True for i in range(0, len(class_target)) if class_pred[i] != 0])\n",
    "    if p==0:p=1\n",
    "    n = len(class_pred) - p\n",
    "\n",
    "    acc = num_cor/len(class_target)\n",
    "    prec = tp / p\n",
    "    rec = tp / (fn+p)\n",
    "    f1_score = 2*prec*rec / (prec+rec)\n",
    "    min_loss = min(losses)\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    print(\"Precision: {}\".format(prec))\n",
    "    print(\"Recall: {}\".format(rec))\n",
    "    print(\"F1_score: {}\".format(f1_score))\n",
    "    print(\"Min loss: {}\".format(min_loss))\n",
    "    print(\"{}/{} coorect\".format(num_cor,len(class_target)))\n",
    "    idx = list(range(0, len(losses)))\n",
    "    plt.plot(idx, losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqfUNm_OmNPe"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=12, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Axy8UglMufDy"
   },
   "outputs": [],
   "source": [
    "tabnet_kfold = TabNet(len(features_full), 3, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "bneZTHZL4dk2",
    "outputId": "eebd404a-965e-4d4c-809b-398cc1f0b966"
   },
   "outputs": [],
   "source": [
    "all_loss_kfold = []\n",
    "for train_idx, val_idx in kf.split(train_tensor): #kfold\n",
    "    optimizer = optim.Adam(tabnet_kfold.parameters(),lr=0.007,weight_decay=0.00001)\n",
    "\n",
    "    new_data_train, new_data_val = train_tensor[train_idx], train_tensor[val_idx]\n",
    "    new_pred_train, new_pred_val = train_targets[train_idx], train_targets[val_idx]\n",
    "    for i in tqdm(range(100)): #kind of brute force, nice acc\n",
    "        tabnet_kfold.zero_grad()\n",
    "        pred = tabnet_kfold(new_data_train.to(device = device))\n",
    "        loss = loss_f(pred.float(), new_pred_train.to(device = device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_pred = tabnet_kfold(new_data_val.to(device = device))\n",
    "    all_loss_kfold.append(loss_f(val_pred, new_pred_val.to(device = device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN5JEFi1_o8R"
   },
   "outputs": [],
   "source": [
    "evaluation(tabnet_kfold, all_loss_kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlD8oAG0gt_e"
   },
   "outputs": [],
   "source": [
    "tabnet_normal = TabNet(len(features_full), 3, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7ekoJSZufD1"
   },
   "outputs": [],
   "source": [
    "all_loss_normal = []\n",
    "for i in tqdm(range(60)): #kind of brute force, nice acc\n",
    "    optimizer = optim.Adam(tabnet_normal.parameters(),lr=0.007,weight_decay=0.00001)\n",
    "    tabnet_normal.zero_grad()\n",
    "    pred = tabnet_normal(train_tensor)\n",
    "    loss = loss_f(pred.float(), train_targets.to(device = device))\n",
    "    all_loss_normal.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpeH8TIVOQdz"
   },
   "outputs": [],
   "source": [
    "evaluation(tabnet_normal, all_loss_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObygkPusag6i"
   },
   "outputs": [],
   "source": [
    "tabnet_batch = TabNet(len(features_full), 3, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bzibw3Q9HZc"
   },
   "outputs": [],
   "source": [
    "all_loss_batch = []\n",
    "batch_size = 7500 #mini batching\n",
    "chunk_size = train_tensor.shape[0]//batch_size\n",
    "pred_chunks = train_targets.chunk(chunk_size, dim=0)\n",
    "optimizer = optim.Adam(tabnet_batch.parameters(),lr=0.007,weight_decay=0.00001)\n",
    "\n",
    "for i in tqdm(range(125)):\n",
    "    batch_loss =  0\n",
    "    for j,chunk in enumerate(train_tensor.chunk(chunk_size, dim=0)):\n",
    "        optimizer.zero_grad()\n",
    "        pred = tabnet_batch(chunk.to(device))\n",
    "        loss = loss_f(pred.float(), pred_chunks[j].to(device = device))\n",
    "        batch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    all_loss_batch.append(batch_loss/chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfhxmB6kufD1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation(tabnet_batch, all_loss_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu2xsLLe7brW"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPEOrS4gyfhk"
   },
   "outputs": [],
   "source": [
    "tabnet_batch_year = TabNet(len(features_full), 3, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HdS7E4n2dfh"
   },
   "outputs": [],
   "source": [
    "years_to_train_batch = list(data_by_year.keys())\n",
    "years_to_train_batch.remove(2020)\n",
    "optimizer = optim.Adam(tabnet_batch.parameters(),lr=0.007,weight_decay=0.00001)\n",
    "all_loss_batch_year=[]\n",
    "for i in tqdm(range(100)):\n",
    "    batch_loss = 0\n",
    "    # random.shuffle(years_to_train_batch)\n",
    "    for year in years_to_train_batch:\n",
    "        training_data_year = torch.from_numpy(np.array(data_by_year[year])).float()\n",
    "        training_target_year = torch.from_numpy(np.array(data_by_year_tar[year])).long().view(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = tabnet_batch_year(training_data_year.to(device))\n",
    "        loss = loss_f(pred.float(), training_target_year.to(device = device))\n",
    "        batch_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    all_loss_batch_year.append(batch_loss/len(years_to_train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsnPCYKA6rBy"
   },
   "outputs": [],
   "source": [
    "evaluation(tabnet_batch_year, all_loss_batch_year)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NBA_TABNET.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
